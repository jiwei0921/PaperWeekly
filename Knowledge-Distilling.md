# Distilling the Knowlege in a Neural Network

Geoffrey Hinton et al. Google Inc.

## abstract 

A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.

要提高任何机器学习算法的性能，一个非常简单的方法就是，在相同的数据上训练很多模型，然后将其预测平均。不幸的是，使用很多模型的集成进行预测是很麻烦的，在大量用户中部署，运算量也非常大，尤其是，如果单个模型也是大型神经网络的话。Caruana等证明了，将集成模型的知识压缩到一个模型是可能的，这部署起来就容易多了，我们进一步发展这个方法，使用了不同的压缩技术。我们在MNIST上取得了一些不错的结果，我们证明了，通过将集成模型中的知识蒸馏到单个模型中，我们将一个使用很多的商用系统的语音模型的性能进行了明显改进。我们还提出了一种新型集成方法，由一个或多个完整模型，和多个专用模型，这些专用模型学习区别细粒度类别，这是完整模型所不具备的。与很多专家的混合不同，这些专用模型可以进行快速的并行训练。

## 1 Introduction 引言

Many insects have a larval form that is optimized for extracting energy and nutrients from the environment and a completely different adult form that is optimized for the very different requirements of traveling and reproduction. In large-scale machine learning, we typically use very similar models for the training stage and the deployment stage despite their very different requirements: For tasks like speech and object recognition, training must extract structure from very large, highly redundant datasets but it does not need to operate in real time and it can use a huge amount of computation. Deployment to a large number of users, however, has much more stringent requirements on latency and computational resources. The analogy with insects suggests that we should be willing to train very cumbersome models if that makes it easier to extract structure from the data. The cumbersome model could be an ensemble of separately trained models or a single very large model trained with a very strong regularizer such as dropout [9]. Once the cumbersome model has been trained, we can then use a different kind of training, which we call “distillation” to transfer the knowledge from the cumbersome model to a small model that is more suitable for deployment. A version of this strategy has already been pioneered by Rich Caruana and his collaborators [1]. In their important paper they demonstrate convincingly that the knowledge acquired by a large ensemble of models can be transferred to a single small model.

很多昆虫都有幼虫形态，这是为从环境中汲取能量和营养而优化的形态，还有一个完全不同的成年形态，这是为很不同的需求而优化的，即旅行和繁殖。在大规模机器学习中，我们一般使用非常类似的模型进行训练和部署，但其需求是很不一样的：对于语音和目标识别这样的任务， 训练必须从非常大型的、高度荣誉的数据集中提取结构，但不需要实时进行，可以进行大量的计算。但部署到大量用户中，对于延迟和计算资源消耗的要求则要严格的多。与昆虫的类比说明，我们应当去训练非常复杂的模型，如果这可以使得从数据中提取结构更容易一些。这个很复杂的模型可以是单独训练的模型的集成，或使用非常强的正则化训练出来的一个非常复杂的模型，如dropout正则化。一旦复杂模型训练好，我们就可以使用不同类型的训练，我们称之为蒸馏，来将知识从复杂模型迁移到小模型中，这样更易于部署。Caruana等已经研究来这种策略的一个版本。在其重要的论文中，他们证明了，很多模型的集成，其中的知识可以迁移到单个小型模型中。

A conceptual block that may have prevented more investigation of this very promising approach is that we tend to identify the knowledge in a trained model with the learned parameter values and this makes it hard to see how we can change the form of the model but keep the same knowledge. A more abstract view of the knowledge, that frees it from any particular instantiation, is that it is a learned mapping from input vectors to output vectors. For cumbersome models that learn to discriminate between a large number of classes, the normal training objective is to maximize the average log probability of the correct answer, but a side-effect of the learning is that the trained model assigns probabilities to all of the incorrect answers and even when these probabilities are very small, some of them are much larger than others. The relative probabilities of incorrect answers tell us a lot about how the cumbersome model tends to generalize. An image of a BMW, for example, may only have a very small chance of being mistaken for a garbage truck, but that mistake is still many times more probable than mistaking it for a carrot.

这种很有希望的方法值得更多研究，但一个概念上的阻碍是，我们倾向于认为训练好的模型中的知识是学习到的参数，这就很难看出，我们改变模型的形式，如何才能保持相同的知识呢？知识的更抽象视角，将其从任何特定的实例中解放出来，是一个学习到的映射，从输入矢量到输出矢量。对于复杂的模型，学习区分大量类别，正常的训练目标是最大化正确答案的平均log概率，但学习的一个副作用是，训练好的模型对所有的不正确答案也指定了概率，即使其中一些的概率值非常小，而一些则比其他的要大很多。不正确答案的相对概率告诉我们很多关于复杂模型泛化的东西。比如一幅BMW的图像，被误认为是一个垃圾卡车的概率很小，但这个概率比误认为是一个胡萝卜的概率还是要大很多的。

It is generally accepted that the objective function used for training should reflect the true objective of the user as closely as possible. Despite this, models are usually trained to optimize performance on the training data when the real objective is to generalize well to new data. It would clearly be better to train models to generalize well, but this requires information about the correct way to generalize and this information is not normally available. When we are distilling the knowledge from a large model into a small one, however, we can train the small model to generalize in the same way as the large model. If the cumbersome model generalizes well because, for example, it is the average of a large ensemble of different models, a small model trained to generalize in the same way will typically do much better on test data than a small model that is trained in the normal way on the same training set as was used to train the ensemble.

大家都承认，训练所用的目标函数，应当尽可能反应用户的目标。尽管这样，模型的训练一般是为在训练数据上的性能优化的，而真正的目标是在新数据上也要泛化良好。能够训练泛化能力更好的模型是最理想的，但这需要正确泛化的信息，一般这样的信息都不可用。但当我们从大型模型蒸馏知识到小型模型时，我们训练小模型泛化的方式，就和大模型一样。如果复杂模型泛化性能良好，比如因为它是很多不同模型的集成，小型模型的训练，其泛化的方式一样，其在测试数据上的表现，会比下面的小型模型要好很多，这个小型模型在相同的训练集上进行正常的训练，和训练集成模型一样。

An obvious way to transfer the generalization ability of the cumbersome model to a small model is to use the class probabilities produced by the cumbersome model as “soft targets” for training the small model. For this transfer stage, we could use the same training set or a separate “transfer” set. When the cumbersome model is a large ensemble of simpler models, we can use an arithmetic or geometric mean of their individual predictive distributions as the soft targets. When the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases, so the small model can often be trained on much less data than the original cumbersome model and using a much higher learning rate.

复杂模型泛化能力的迁移到小型模型中，一个明显的方法是使用复杂模型生成的类别概率来作为软目标，来训练小型模型。在这个迁移阶段中，我们可以使用相同的训练集，或一个分离的迁移集。当复杂模型是很多简单模型的集成时，我们可以用其单独预测分布的代数或几何平均作为软目标。当软目标的熵很高时，每个训练案例提供的信息会比硬目标更多，训练案例之间梯度的方差也会更小，所以小型模型的训练可以在很少的数据上进行，也可以使用大的多的学习速率。

For tasks like MNIST in which the cumbersome model almost always produces the correct answer with very high confidence, much of the information about the learned function resides in the ratios of very small probabilities in the soft targets. For example, one version of a 2 may be given a probability of 10^−6 of being a 3 and 10^−9 of being a 7 whereas for another version it may be the other way around. This is valuable information that defines a rich similarity structure over the data (i. e. it says which 2’s look like 3’s and which look like 7’s) but it has very little influence on the cross-entropy cost function during the transfer stage because the probabilities are so close to zero. Caruana and his collaborators circumvent this problem by using the logits (the inputs to the final softmax) rather than the probabilities produced by the softmax as the targets for learning the small model and they minimize the squared difference between the logits produced by the cumbersome model and the logits produced by the small model. Our more general solution, called “distillation”, is to raise the temperature of the final softmax until the cumbersome model produces a suitably soft set of targets. We then use the same high temperature when training the small model to match these soft targets. We show later that matching the logits of the cumbersome model is actually a special case of distillation.

对于MNIST这样的任务，复杂模型可以以非常高的置信度几乎得到完全正确的答案，学习到的函数，很多信息都在软目标的很小概率的部分中。比如，一个版本的2，会有10^-6的概率为3，有10^-9的概率为7，但另一个版本可能是反过来的。这对于定义丰富的相似性结构，是很有用的信息（即，哪些2像3，哪些2像7），但对于迁移阶段的交叉熵损失影响很小，因为其概率太接近0。Caruana等通过使用logits（到最终softmax的输入）而不是softmax生成的概率，作为小型模型学习的目标，来最小化复杂模型生成的logits和小型模型生成的logits之间的平方误差，解决来这个问题。我们更一般的解决方法，称为蒸馏，是提高最终softmax的温度，直到复杂模型合适的软目标集。我们然后使用同样的高温进行小型模型的训练，以匹配这些软目标。我们后面会证明，这个与复杂模型logits的匹配，实际上是蒸馏的一种特殊形式。

The transfer set that is used to train the small model could consist entirely of unlabeled data [1] or we could use the original training set. We have found that using the original training set works well, especially if we add a small term to the objective function that encourages the small model to predict the true targets as well as matching the soft targets provided by the cumbersome model. Typically, the small model cannot exactly match the soft targets and erring in the direction of the correct answer turns out to be helpful.

用于训练小型模型的迁移集，可以包含完全未标注的数据，或者也可以使用原始的训练集。我们发现，使用原始训练集效果不错，尤其是我们给目标函数增加了一个小项，鼓励小型模型在预测真实目标的同时，与复杂模型提供的软目标相匹配。一般来说，小型模型不能完全匹配软目标，在正确答案的方向上有一些错误结果是好的。

## 2 Distillation

Neural networks typically produce class probabilities by using a “softmax” output layer that converts the logit, zi, computed for each class into a probability, qi, by comparing zi with the other logits.

神经网络一般使用softmax输出层来生成类别概率，将每个类别的logit zi，与其他logits比较，转化成类别qi

$$q_i = \frac {exp(z_i/T)} {\sum_j exp(z_j/T)}$$(1)

where T is a temperature that is normally set to 1. Using a higher value for T produces a softer probability distribution over classes. 其中T是一个温度，通常情况下设为1。使用较高的温度T会生成各类上更软的概率分布。

In the simplest form of distillation, knowledge is transferred to the distilled model by training it on a transfer set and using a soft target distribution for each case in the transfer set that is produced by using the cumbersome model with a high temperature in its softmax. The same high temperature is used when training the distilled model, but after it has been trained it uses a temperature of 1.

蒸馏的最简单形式中，通过在迁移集上训练模型，对迁移集中的各种情况使用软目标分布，就可以将知识迁移到蒸馏的模型中，软目标分布的生成，是对较高温度softmax的复杂模型。在训练蒸馏模型时，也使用相同的高温，但在训练过后，则使用温度为1的值。

When the correct labels are known for all or some of the transfer set, this method can be significantly improved by also training the distilled model to produce the correct labels. One way to do this is to use the correct labels to modify the soft targets, but we found that a better way is to simply use a weighted average of two different objective functions. The first objective function is the cross entropy with the soft targets and this cross entropy is computed using the same high temperature in the softmax of the distilled model as was used for generating the soft targets from the cumbersome model. The second objective function is the cross entropy with the correct labels. This is computed using exactly the same logits in softmax of the distilled model but at a temperature of 1. We found that the best results were generally obtained by using a condiderably lower weight on the second objective function. Since the magnitudes of the gradients produced by the soft targets scale is 1/T^2 it is important to multiply them by T^2 when using both hard and soft targets. This ensures that the relative contributions of the hard and soft targets remain roughly unchanged if the temperature used for distillation is changed while experimenting with meta-parameters.

迁移集中的部分/全部正确标签都已知时，通过训练蒸馏模型来生成正确标签，可以显著改进这种方法的效果。一种方法是用正确的标签来修正软目标，但我们发现更好的方法是，简单的使用两种不同的目标函数的加权平均。第一个目标函数是软目标的交叉熵，这个交叉熵使用的是相同的高温，和蒸馏模型的softmax中的高温一样，和从复杂模型中生成软目标使用的一样。第二个目标函数是使用正确标签的交叉熵。这是使用与蒸馏模型softmax中相同的logits计算得到的，但温度为1。我们发现要得到最好的结果，在第二个目标函数上要使用小很多的权重。因为软目标生成的梯度的幅值为1/T^2，所以在同时使用硬目标和软目标的时候，需要将其乘以T^2。这确保了，在使用元参数时，如果蒸馏所用的温度变化时，硬目标和软目标的贡献保持大致不变。

### 2.1 Matching logits is a special case of distillation

Each case in the transfer set contributes a cross-entropy gradient, dC/dz_i, with respect to each logit, z_i of the distilled model. If the cumbersome model has logits v_i which produce soft target probabilities pi and the transfer training is done at a temperature of T , this gradient is given by:

迁移集中的每种情况都贡献了一个交叉熵梯度，dC/dz_i，这是对蒸馏模型每个logit z_i的。如果复杂模型